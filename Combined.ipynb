{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"text-align: center;\">EE 371K Data Science Laboratory final report</p>\n",
    "## <p style=\"text-align: center;\">Favorita Grocery Sales Forecasting</p>\n",
    "\n",
    "## <p style=\"text-align: center;\">Fenglong Cai | Kyle Bradford  | Spencer Yue | Yiming Liao</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "### 1. Introduction\n",
    "\n",
    "Brick-and-mortar grocery stores are always in a delicate dance with purchasing and sales forecasting. Predict a little over, and grocers are stuck with overstocked, perishable goods. Guess a little under, and popular items quickly sell out, leaving money on the table and customers fuming.\n",
    "\n",
    "Our project，which is based on the ongoing kaggle competition, is aim to predict the unit sales for thousands of items sold at different Favorita stores located in Ecuador. If we can build a model that more accurately forecasts product sales, we can better ensure the grocery stores please customers by having just enough of the right products at the right time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Analysis\n",
    "\n",
    "Below is the structure of our data.We have six data file contains information from three dimension: store, item and date.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://favorita-ee379k.github.io/MA_graph/data_structure.png\"\n",
    "style=\"width:500px;height:310px;float\">\n",
    "\n",
    "\n",
    "Training data includes the target unit_sales by date, store_nbr, and item_nbr.From the stores' number, items' number and date we can find supplementary information，such as store location，item class and oil price，from other data file.The supplementary information that may be useful in building your models.\n",
    "\n",
    "Before predicting the unit sales, we first did data analysis to gain some useful information.\n",
    "\n",
    "We focused on the information of store at first step.The most important information of store is total transactions. Total transactions is the sum of all items' unit sales in a store. We could select valuable features based on the total transactions  \n",
    "\n",
    "<img src=\"https://favorita-ee379k.github.io/MA_graph/store_transaction.png\"\n",
    "style=\"width:600px;height:310px;float\">\n",
    "\n",
    "We found that the variance of total transactions of the stores in same cluster or type is small. So we think the cluster and type of stores are really important features of stores.\n",
    "\n",
    "Then we try to explore the influence of holiday on the total transaction. \n",
    "\n",
    "<img src=\"https://favorita-ee379k.github.io/MA_graph/holiday.png\"\n",
    "style=\"width:600px;height:310px;float\">\n",
    "\n",
    "We found that,generally, the transactions of the day before holidays are larger than the original days. We can found that from the orange nodes in the following picture.However, the data we try to predict are on August. There is only one holiday on August which is a special case，the transactions of the day before this holiday is as usual.\n",
    "\n",
    "Then we turn to the oil price. We see the correlation between the oil price and the total transactions of different stores. Then the results showed that oil price have little influence on the total transactions.  \n",
    "\n",
    "<img src=\"https://favorita-ee379k.github.io/MA_graph/oil.png\"\n",
    "style=\"width:400px;height:310px;float\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving Average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A moving average (rolling average or running average) is a calculation to analyze data points by creating series of averages of different subsets of the full data set.\n",
    "\n",
    "In this problem, we focused on the unit sales and date ignoring other supplementary information for moving average model. We group the data by their item number and store number so that get a series of data for a item in one store.Just as the following figure show:\n",
    "\n",
    "<img src=\"https://favorita-ee379k.github.io/MA_graph/series.png\"\n",
    "style=\"width:500px;height:310px;float\">\n",
    "\n",
    "Then we averaged their unit sales to get the prediction of unit sales to test data. And we selected different time slot including 1 day ago 7 day ago and so on,to do the average operation. We created series of averages for different time slot. Then we choose the median value among those average value as our prediction.  \n",
    "\n",
    "There is part of code of moving average:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ma_is = train[['item_nbr','store_nbr','unit_sales']].groupby(['item_nbr','store_nbr'])['unit_sales'].mean().to_frame('mais226')\n",
    "for i in [112,56,28,14,7,3,1]:\n",
    "    tmp = train[train.date>lastdate-timedelta(int(i))]\n",
    "    tmpg = tmp.groupby(['item_nbr','store_nbr'])['unit_sales'].mean().to_frame('mais'+str(i))\n",
    "    ma_is = ma_is.join(tmpg, how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Autoregression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline AR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a baseline for how well autoregression should perform, we tried doing autoregression directly on unit sales for each item. For this, we simply used statsmodels' AR model, and trained it on item unit sales. The kaggle test set is 16 days, from August 16, 2017 to August 31, 2017. So, to get an idea of how this model would perform, we first trained it on all the data in the train set except the last 16 days, then predicted those last 16 days. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some prediction results compared to the truth:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://favorita-ee379k.github.io/autoregression/graphs/baseline-ar-pred-comparison.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best score we were able to get on Kaggle when running this AR model on the true test set was 0.662, training on a 365 day history instead of all the available train set. As a comparison, the baseline score provided on Kaggle was .911, a score obtained by using the previous year's sales as a prediction. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Level AR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the data, we noticed that on a per store level, the periodicity of the unit sales is much more visible and easier to predict compared to unit sales on a per item level. Many items average very low unit sales, so the signal is much lower resolution and is more sporadic. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some sales totals for stores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://favorita-ee379k.github.io/autoregression/graphs/store_sales.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some sales totals for items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://favorita-ee379k.github.io/autoregression/graphs/item_sales.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we hypothesized that we may be missing out on some information about the overall trends by trying to predict item unit sales. To try to alleviate this issue, we decided to split unit sales data into 4 hierarchical categories, and train/predict on them separately. In our data, each item has a specified family and class, so we decided to use that to split the data categorically. In total, there are 4100 unique items, split into 337 unique classes, which are assigned to 33 unique families.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of the data available for each item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://favorita-ee379k.github.io/autoregression/graphs/item_dataframe.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So after processing the data, we have 4 new datasets to train and predict on:\n",
    "<ol>\n",
    "    <li><b>Total unit sales per store</b></li>\n",
    "    <li><b>Proportion of family sales per store</b></li>\n",
    "    <li><b>Proportion of class sales per family</b></li>\n",
    "    <li><b>Proportion of item unit sales per class</b></li>\n",
    "</ol>\n",
    "\n",
    "Then to transform our predictions back to the original target of unit sales per item, we can simply multiply these 4 predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 1: Some unit sales per store predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://favorita-ee379k.github.io/autoregression/graphs/store_sales_preds.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the next 3 datasets, we were predicting proportions in the range [0, 1], so we trained on a logit transform $log(\\frac{p}{1-p})$ of the data. This way, when the predictions are transformed back with $\\frac{e^{pred}}{1-e^{pred}}$, they are limited to the (0, 1) range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 2: Some predictions for family proportion of store sales or $\\frac{family\\_sales}{store\\_sales}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://favorita-ee379k.github.io/autoregression/graphs/family_proportion_preds.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 3: Some predictions for class proportion of family sales or $\\frac{class\\_sales}{family\\_sales}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://favorita-ee379k.github.io/autoregression/graphs/class_proportion_preds.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 4: Some predictions of item proportion of class sales or $\\frac{item\\_sales}{class\\_sales}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://favorita-ee379k.github.io/autoregression/graphs/item_proportion_predictions.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To transform these predictions into a prediction for unit sales per item, we do $store\\_sales * \\frac{family\\_sales}{store\\_sales} * \\frac{class\\_sales}{family\\_sales} * \\frac{item\\_sales}{class\\_sales} = item\\_sales$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some unit sales predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://favorita-ee379k.github.io/autoregression/graphs/multi-level-ar-pred-comparison.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing these plots to the plots from the baseline AR predictions, it is not immediately apparent that the multi-level model is performing better. The multi-level autoregression did better on some items, but worse on others. However, we were able to score a .588 on Kaggle using this method, a \\~16% improvement over the .662 that the baseline model scored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the two models (error per sample over 16 day prediction period)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://favorita-ee379k.github.io/autoregression/graphs/all-baseline-vs-ml-errors.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://favorita-ee379k.github.io/autoregression/graphs/ml-errors-minus-baseline-errors.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above graphs, it is clear to see that the multi-level AR predictions were better on average. The baseline AR predictions have some very high outliers when looking at errors for individual items, some as high as 35. In comparison, the multi-level AR's worst errors for individual items are close to 5. And when taking the difference of errors $ multi\\ level\\ errors\\ per\\ sample - baseline\\ errors\\ per\\ sample$, the result is usually negative, meaning the multi-level model tends to have a lower error for a given sample. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can we combine the best predictions from both to get a better prediction?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing average error per store difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://favorita-ee379k.github.io/autoregression/graphs/ml-errors-minus-baseline-errors-per-store.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing average error per item difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://favorita-ee379k.github.io/autoregression/graphs/ml-errors-minus-baseline-errors-per-item.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the errors, there are a few stores and some items that the baseline model tends to do better on. Again, these errors are from using the last 16 days of the Kaggle train set as a test set. So we tried using the best store results and the best item results based on these errors. The hope was that if a model did better on a given store or item in our test, it will also do better for that same store or item in the real test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, in practice, keeping the best predictions based on average per store error gave a .600 and keeping the best predictions based on average per item error gave .594, neither an improvement over the multi-level AR model alone. Since the best store errors and best item errors were only based on one test set, we were probably overfitting to our test set. Comparing many predictions for different places in the training set could probably give a better idea of which model performs best on which store/item."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion for autoregression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With autoregression, we were able to significantly beat the baseline score .911 calculated from last years' sales, obtaining a .588 score on Kaggle at best. However, we were getting much better results with other models, so we did not explore autoregression further than this, and instead focused on improving our other models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Machine Learning Method\n",
    "\n",
    "All the models above can handle the time feature well, but cannot insert all those categorical features. In order to do that, we come to machine learning method for help. When applying those method, feature engineering and computation speed becomes extremely important.\n",
    "\n",
    "### 1. First Try\n",
    "\n",
    "Handling time series in traditional mathine learning method is rather tricky. We first dummy the date into year, month, the number of the week as well as weekday, and feed all the unit sale data into the models from 20160801 to 20170815. Besides, we dummy all the categorical features such as the state, city, store type and cluster and use them as new features. The model we've tired in this part including Random Forest, Bagging, XGBoost as well as MLP. The plot below descirbes the feature performance.\n",
    "\n",
    "![performance](https://favorita-ee379k.github.io/MLgraph/performance.png)\n",
    "\n",
    "From the picture, we can see that XGBoost is still the best among the four machine learning models but they all get a score of more than 1, twice as large as the Moving Average's score and even larger than the last year model. The poor performance is because of the feature enginnering we made was not good. The date itself does not contain enough information, and feeding all the raw time series data will lead to overfitting. Besides, these four models need a huge amount of time to tune and train given the amount of training data. We need to find a faster model first before further feature Engineering.\n",
    "\n",
    "### 2. LGBM is faster\n",
    "\n",
    "LGBM is short for Light Gradient Boosting Method. Different from other boosing methods, this model splits the trees leaf-wise instead of level-wise, this allows LGBM to minimise loss faster and thus gain more computation speed. The pictures below describes the concept well.\n",
    "\n",
    "![leaf-wise](https://favorita-ee379k.github.io/MLgraph/leaf-wise.png)\n",
    "![level-wise](https://favorita-ee379k.github.io/MLgraph/level-wise.png)\n",
    "\n",
    "With the help of LGBM, we get more time and more patience to do more feature engineering.\n",
    "\n",
    "### 3. Feature Engineering\n",
    "\n",
    "With the inspiration of moving average and smoothing method, we begin to view time series as a combination of trend, seasonality and noise. We want to capture the trend and seasonality instead of noise. First, for the trend, we calculate the average unit sales of each item in a certain store. The time period we used was firstly 7 days, 14 days, 28 days as well as 56 days, since this will capture the weekly mean as well as monthly trend. But after we did a FFT on the data we also discovered a period of 2 days and 3 days, so we add the 2-day mean and 3-day mean into our model too. We use these moving-average features to predict the following 16 days since the task is to predict the last 16 days' sale in August. In that way we capture the trend well. Next, to capture the seasonality, the 16 days of training data all begins on Wednesday, the same with the first day of the test set. We fit 16 LGBM model individually to the 16 days in our prepared training dataset. In this way we capture the short term seasonality well. However, in this way, our model only used the most information of 16 days and a small amount of information of 56 days. The way we use information is still insufficient. In order to cope with that, we created 4 training sets, begining at 6.28, 7.5, 7.12 and 7.19. For the yearly trend, we just ignored it, for we tried the same thing on 2016 and 2017 but after ensemble them the result got worse.\n",
    "\n",
    "For transaction features, we used 7 day mean, 14 day mean and 28 day mean. For categorical features, we used store city and store cluster as well as onpromotion and amount of ompromotion in last 14 days. We also tried other features like oil price but it was total useless and ruined our result. The structure of feature engineering are shown below.\n",
    "\n",
    "![feature_structure](https://favorita-ee379k.github.io/MLgraph/feature_structure.png)\n",
    "\n",
    "The feature importances of the first day and the last day in the training set are shown below.\n",
    "\n",
    "![day_1](https://favorita-ee379k.github.io/MLgraph/day_1.png)\n",
    "![day_2](https://favorita-ee379k.github.io/MLgraph/day_16.png)\n",
    "\n",
    "The following table allows to view the order change of the 16 days more clearly.\n",
    "![feature_importance](https://favorita-ee379k.github.io/MLgraph/feature_importance.png)\n",
    "We can find that as the number of days goes up, 7 day mean's importance decreases while 56 day mean's importance increases. This indicating that as when the predicting date is far away from our training data, the importance of long term trend goes up while the importance of short term trend goes down.\n",
    "\n",
    "With this model, we reached a score of 0.520. After ensembled it with the result from moving average, we reached a score of 0.517, ranking 38/1126 in the competition.\n",
    "![rank](https://favorita-ee379k.github.io/MLgraph/rank.png)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
