









<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <title>Favorita Grocery Sales Forecasting</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="">
  <meta name="author" content="">

  <meta name="robots" content="noindex,nofollow">


  <!-- Le styles -->
  <script src="http://nbviewer.jupyter.org/cdn-cgi/apps/head/MuIIl4I_IVFkxldaVu1mdWee9as.js"></script><link href="http://nbviewer.jupyter.org/static/build/styles.css?v=d2f574aebb7720ccb35207ba1db4cfbb" rel="stylesheet">

  <!-- Le HTML5 shim, for IE6-8 support of HTML5 elements -->
  <!--[if lt IE 9]>
    <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
  <![endif]-->

  <!-- Le fav and touch icons -->
  <link rel="shortcut icon" href="http://nbviewer.jupyter.org/static/ico/ipynb_icon_16x16.png">
  <link rel="apple-touch-icon-precomposed" sizes="144x144"
        href="http://nbviewer.jupyter.org/static/ico/apple-touch-icon-144-precomposed.png?v=5a3c9ede93e2a8b8ea9e3f8f3da1a905">
  <link rel="apple-touch-icon-precomposed" sizes="114x114"
        href="http://nbviewer.jupyter.org/static/ico/apple-touch-icon-114-precomposed.png?v=45d86fc8f24dc00638035e1dd7a6d898">
  <link rel="apple-touch-icon-precomposed" sizes="72x72"
        href="http://nbviewer.jupyter.org/static/ico/apple-touch-icon-72-precomposed.png?v=540b5eb0f3cfd25f1439d1c9bd30e15f">
  <link rel="apple-touch-icon-precomposed"
        href="http://nbviewer.jupyter.org/static/ico/apple-touch-icon-57-precomposed.png?v=225f0590e187e1458625654f10a28f56">

    <link href="http://nbviewer.jupyter.org/static/build/notebook.css?v=7ff1df8ccee6dd13babbd66843869949" rel="stylesheet">





    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML" type="text/javascript">
    </script>
    <script type="text/javascript">
      init_mathjax = function() {
        if (window.MathJax) {
          // MathJax loaded
          MathJax.Hub.Config({
            TeX: {
              equationNumbers: {
                autoNumber: "AMS",
                useLabelIds: true
              }
            },
            tex2jax: {
              inlineMath: [ ['$','$'], ["\\(","\\)"] ],
              displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
              processEscapes: true,
              processEnvironments: true
            },
            displayAlign: 'center',
            "HTML-CSS": {
              styles: {'.MathJax_Display': {"margin": 0}},
              linebreaks: { automatic: true }
            }
          });
          MathJax.Hub.Queue(["Typeset", MathJax.Hub]);
        }
      }
      init_mathjax();
    </script>


</head>

<body class="nbviewer">

  <!-- These are loaded at the top of the body so they are available to
       notebook cells when they are loaded below. -->
  <script src="http://nbviewer.jupyter.org/static/components/jquery/dist/jquery.min.js?v=c9f5aeeca3ad37bf2aa006139b935f0a"></script>
  <script src="http://nbviewer.jupyter.org/static/components/requirejs/require.js?v=6da8be361b9ee26c5e721e76c6d4afce"></script>
  <script src="http://nbviewer.jupyter.org/static/components/moment/min/moment.min.js?v=89f87298ad94aa1e6b92f42eb66da043"></script>

  <div class="container container-main">


  <div id="notebook">
    <div id="notebook-container">

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="EE-371K-Data-Science-Laboratory-final-report"><p style="text-align: center;">EE 371K Data Science Laboratory final report</p><a class="anchor-link" href="#EE-371K-Data-Science-Laboratory-final-report">&#182;</a></h1><h2 id="Favorita-Grocery-Sales-Forecasting"><p style="text-align: center;">Favorita Grocery Sales Forecasting</p><a class="anchor-link" href="#Favorita-Grocery-Sales-Forecasting">&#182;</a></h2><h2 id="Fenglong-Cai-|-Kyle-Bradford--|-Spencer-Yue-|-Yiming-Liao"><p style="text-align: center;">Fenglong Cai | Kyle Bradford  | Spencer Yue | Yiming Liao</p><a class="anchor-link" href="#Fenglong-Cai-|-Kyle-Bradford--|-Spencer-Yue-|-Yiming-Liao">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="1.-Introduction">1. Introduction<a class="anchor-link" href="#1.-Introduction">&#182;</a></h3><p>Brick-and-mortar grocery stores are always in a delicate dance with purchasing and sales forecasting. Predict a little over, and grocers are stuck with overstocked, perishable goods. Guess a little under, and popular items quickly sell out, leaving money on the table and customers fuming.</p>
<p>Our project，which is based on the ongoing kaggle competition, is aim to predict the unit sales for thousands of items sold at different Favorita stores located in Ecuador. If we can build a model that more accurately forecasts product sales, we can better ensure the grocery stores please customers by having just enough of the right products at the right time.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="2.-Data-Analysis">2. Data Analysis<a class="anchor-link" href="#2.-Data-Analysis">&#182;</a></h3><p>Below is the structure of our data.We have six data file contains information from three dimension: store, item and date.</p>
<p><img src="https://favorita-ee379k.github.io/MA_graph/data_structure.png"
style="width:500px;height:310px;float"></p>
<p>Training data includes the target unit_sales by date, store_nbr, and item_nbr.From the stores' number, items' number and date we can find supplementary information，such as store location，item class and oil price，from other data file.The supplementary information that may be useful in building your models.</p>
<p>Before predicting the unit sales, we first did data analysis to gain some useful information.</p>
<p>We focused on the information of store at first step.The most important information of store is total transactions. Total transactions is the sum of all items' unit sales in a store. We could select valuable features based on the total transactions</p>
<p><img src="https://favorita-ee379k.github.io/MA_graph/store_transaction.png"
style="width:600px;height:310px;float"></p>
<p>We found that the variance of total transactions of the stores in same cluster or type is small. So we think the cluster and type of stores are really important features of stores.</p>
<p>Then we try to explore the influence of holiday on the total transaction.</p>
<p><img src="https://favorita-ee379k.github.io/MA_graph/holiday.png"
style="width:600px;height:310px;float"></p>
<p>We found that,generally, the transactions of the day before holidays are larger than the original days. We can found that from the orange nodes in the following picture.However, the data we try to predict are on August. There is only one holiday on August which is a special case，the transactions of the day before this holiday is as usual.</p>
<p>Then we turn to the oil price. We see the correlation between the oil price and the total transactions of different stores. Then the results showed that oil price have little influence on the total transactions.</p>
<p><img src="https://favorita-ee379k.github.io/MA_graph/oil.png"
style="width:400px;height:310px;float"></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Moving-Average">Moving Average<a class="anchor-link" href="#Moving-Average">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>A moving average (rolling average or running average) is a calculation to analyze data points by creating series of averages of different subsets of the full data set.</p>
<p>In this problem, we focused on the unit sales and date ignoring other supplementary information for moving average model. We group the data by their item number and store number so that get a series of data for a item in one store.Just as the following figure show:</p>
<p><img src="https://favorita-ee379k.github.io/MA_graph/series.png"
style="width:500px;height:310px;float"></p>
<p>Then we averaged their unit sales to get the prediction of unit sales to test data. And we selected different time slot including 1 day ago 7 day ago and so on,to do the average operation. We created series of averages for different time slot. Then we choose the median value among those average value as our prediction.</p>
<p>There is part of code of moving average:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">ma_is</span> <span class="o">=</span> <span class="n">train</span><span class="p">[[</span><span class="s1">&#39;item_nbr&#39;</span><span class="p">,</span><span class="s1">&#39;store_nbr&#39;</span><span class="p">,</span><span class="s1">&#39;unit_sales&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">groupby</span><span class="p">([</span><span class="s1">&#39;item_nbr&#39;</span><span class="p">,</span><span class="s1">&#39;store_nbr&#39;</span><span class="p">])[</span><span class="s1">&#39;unit_sales&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">to_frame</span><span class="p">(</span><span class="s1">&#39;mais226&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">112</span><span class="p">,</span><span class="mi">56</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span><span class="mi">14</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">]:</span>
    <span class="n">tmp</span> <span class="o">=</span> <span class="n">train</span><span class="p">[</span><span class="n">train</span><span class="o">.</span><span class="n">date</span><span class="o">&gt;</span><span class="n">lastdate</span><span class="o">-</span><span class="n">timedelta</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">i</span><span class="p">))]</span>
    <span class="n">tmpg</span> <span class="o">=</span> <span class="n">tmp</span><span class="o">.</span><span class="n">groupby</span><span class="p">([</span><span class="s1">&#39;item_nbr&#39;</span><span class="p">,</span><span class="s1">&#39;store_nbr&#39;</span><span class="p">])[</span><span class="s1">&#39;unit_sales&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">to_frame</span><span class="p">(</span><span class="s1">&#39;mais&#39;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>
    <span class="n">ma_is</span> <span class="o">=</span> <span class="n">ma_is</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">tmpg</span><span class="p">,</span> <span class="n">how</span><span class="o">=</span><span class="s1">&#39;left&#39;</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<h1 id="Autoregression">Autoregression<a class="anchor-link" href="#Autoregression">&#182;</a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Baseline-AR">Baseline AR<a class="anchor-link" href="#Baseline-AR">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>As a baseline for how well autoregression should perform, we tried doing autoregression directly on unit sales for each item. For this, we simply used statsmodels' AR model, and trained it on item unit sales. The kaggle test set is 16 days, from August 16, 2017 to August 31, 2017. So, to get an idea of how this model would perform, we first trained it on all the data in the train set except the last 16 days, then predicted those last 16 days.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Some-prediction-results-compared-to-the-truth:">Some prediction results compared to the truth:<a class="anchor-link" href="#Some-prediction-results-compared-to-the-truth:">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="https://favorita-ee379k.github.io/autoregression/graphs/baseline-ar-pred-comparison.png"></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The best score we were able to get on Kaggle when running this AR model on the true test set was 0.662, training on a 365 day history instead of all the available train set. As a comparison, the baseline score provided on Kaggle was .911, a score obtained by using the previous year's sales as a prediction. <br></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Multi-Level-AR">Multi-Level AR<a class="anchor-link" href="#Multi-Level-AR">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Looking at the data, we noticed that on a per store level, the periodicity of the unit sales is much more visible and easier to predict compared to unit sales on a per item level. Many items average very low unit sales, so the signal is much lower resolution and is more sporadic.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Some-sales-totals-for-stores">Some sales totals for stores<a class="anchor-link" href="#Some-sales-totals-for-stores">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="https://favorita-ee379k.github.io/autoregression/graphs/store_sales.png"></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Some-sales-totals-for-items">Some sales totals for items<a class="anchor-link" href="#Some-sales-totals-for-items">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="https://favorita-ee379k.github.io/autoregression/graphs/item_sales.png"></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>So we hypothesized that we may be missing out on some information about the overall trends by trying to predict item unit sales. To try to alleviate this issue, we decided to split unit sales data into 4 hierarchical categories, and train/predict on them separately. In our data, each item has a specified family and class, so we decided to use that to split the data categorically. In total, there are 4100 unique items, split into 337 unique classes, which are assigned to 33 unique families.
<br></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Example-of-the-data-available-for-each-item">Example of the data available for each item<a class="anchor-link" href="#Example-of-the-data-available-for-each-item">&#182;</a></h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="https://favorita-ee379k.github.io/autoregression/graphs/item_dataframe.png"></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>So after processing the data, we have 4 new datasets to train and predict on:</p>
<ol>
    <li><b>Total unit sales per store</b></li>
    <li><b>Proportion of family sales per store</b></li>
    <li><b>Proportion of class sales per family</b></li>
    <li><b>Proportion of item unit sales per class</b></li>
</ol><p>Then to transform our predictions back to the original target of unit sales per item, we can simply multiply these 4 predictions.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Dataset-1:-Some-unit-sales-per-store-predictions">Dataset 1: Some unit sales per store predictions<a class="anchor-link" href="#Dataset-1:-Some-unit-sales-per-store-predictions">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="https://favorita-ee379k.github.io/autoregression/graphs/store_sales_preds.png"></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>For the next 3 datasets, we were predicting proportions in the range [0, 1], so we trained on a logit transform $log(\frac{p}{1-p})$ of the data. This way, when the predictions are transformed back with $\frac{e^{pred}}{1-e^{pred}}$, they are limited to the (0, 1) range.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Dataset-2:-Some-predictions-for-family-proportion-of-store-sales-or-$\frac{family\_sales}{store\_sales}$">Dataset 2: Some predictions for family proportion of store sales or $\frac{family\_sales}{store\_sales}$<a class="anchor-link" href="#Dataset-2:-Some-predictions-for-family-proportion-of-store-sales-or-$\frac{family\_sales}{store\_sales}$">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="https://favorita-ee379k.github.io/autoregression/graphs/family_proportion_preds.png"></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Dataset-3:-Some-predictions-for-class-proportion-of-family-sales-or-$\frac{class\_sales}{family\_sales}$">Dataset 3: Some predictions for class proportion of family sales or $\frac{class\_sales}{family\_sales}$<a class="anchor-link" href="#Dataset-3:-Some-predictions-for-class-proportion-of-family-sales-or-$\frac{class\_sales}{family\_sales}$">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="https://favorita-ee379k.github.io/autoregression/graphs/class_proportion_preds.png"></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Dataset-4:-Some-predictions-of-item-proportion-of-class-sales-or-$\frac{item\_sales}{class\_sales}$">Dataset 4: Some predictions of item proportion of class sales or $\frac{item\_sales}{class\_sales}$<a class="anchor-link" href="#Dataset-4:-Some-predictions-of-item-proportion-of-class-sales-or-$\frac{item\_sales}{class\_sales}$">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="https://favorita-ee379k.github.io/autoregression/graphs/item_proportion_predictions.png"></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Putting-it-all-together:">Putting it all together:<a class="anchor-link" href="#Putting-it-all-together:">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To transform these predictions into a prediction for unit sales per item, we do $store\_sales * \frac{family\_sales}{store\_sales} * \frac{class\_sales}{family\_sales} * \frac{item\_sales}{class\_sales} = item\_sales$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Some-unit-sales-predictions">Some unit sales predictions<a class="anchor-link" href="#Some-unit-sales-predictions">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="https://favorita-ee379k.github.io/autoregression/graphs/multi-level-ar-pred-comparison.png"></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Comparing these plots to the plots from the baseline AR predictions, it is not immediately apparent that the multi-level model is performing better. The multi-level autoregression did better on some items, but worse on others. However, we were able to score a .588 on Kaggle using this method, a ~16% improvement over the .662 that the baseline model scored.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Comparing-the-two-models-(error-per-sample-over-16-day-prediction-period)">Comparing the two models (error per sample over 16 day prediction period)<a class="anchor-link" href="#Comparing-the-two-models-(error-per-sample-over-16-day-prediction-period)">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="https://favorita-ee379k.github.io/autoregression/graphs/all-baseline-vs-ml-errors.png"></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="https://favorita-ee379k.github.io/autoregression/graphs/ml-errors-minus-baseline-errors.png"></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>From the above graphs, it is clear to see that the multi-level AR predictions were better on average. The baseline AR predictions have some very high outliers when looking at errors for individual items, some as high as 35. In comparison, the multi-level AR's worst errors for individual items are close to 5. And when taking the difference of errors $ multi\ level\ errors\ per\ sample - baseline\ errors\ per\ sample$, the result is usually negative, meaning the multi-level model tends to have a lower error for a given sample.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Can-we-combine-the-best-predictions-from-both-to-get-a-better-prediction?">Can we combine the best predictions from both to get a better prediction?<a class="anchor-link" href="#Can-we-combine-the-best-predictions-from-both-to-get-a-better-prediction?">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Comparing-average-error-per-store-difference">Comparing average error per store difference<a class="anchor-link" href="#Comparing-average-error-per-store-difference">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="https://favorita-ee379k.github.io/autoregression/graphs/ml-errors-minus-baseline-errors-per-store.png"></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Comparing-average-error-per-item-difference">Comparing average error per item difference<a class="anchor-link" href="#Comparing-average-error-per-item-difference">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="https://favorita-ee379k.github.io/autoregression/graphs/ml-errors-minus-baseline-errors-per-item.png"></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>According to the errors, there are a few stores and some items that the baseline model tends to do better on. Again, these errors are from using the last 16 days of the Kaggle train set as a test set. So we tried using the best store results and the best item results based on these errors. The hope was that if a model did better on a given store or item in our test, it will also do better for that same store or item in the real test.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Unfortunately, in practice, keeping the best predictions based on average per store error gave a .600 and keeping the best predictions based on average per item error gave .594, neither an improvement over the multi-level AR model alone. Since the best store errors and best item errors were only based on one test set, we were probably overfitting to our test set. Comparing many predictions for different places in the training set could probably give a better idea of which model performs best on which store/item.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Conclusion-for-autoregression">Conclusion for autoregression<a class="anchor-link" href="#Conclusion-for-autoregression">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>With autoregression, we were able to significantly beat the baseline score .911 calculated from last years' sales, obtaining a .588 score on Kaggle at best. However, we were getting much better results with other models, so we did not explore autoregression further than this, and instead focused on improving our other models.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<h1 id="Machine-Learning-Method">Machine Learning Method<a class="anchor-link" href="#Machine-Learning-Method">&#182;</a></h1><p>All the models above can handle the time feature well, but cannot insert all those categorical features. In order to do that, we come to machine learning method for help. When applying those method, feature engineering and computation speed becomes extremely important.</p>
<h3 id="1.-First-Try">1. First Try<a class="anchor-link" href="#1.-First-Try">&#182;</a></h3><p>Handling time series in traditional mathine learning method is rather tricky. We first dummy the date into year, month, the number of the week as well as weekday, and feed all the unit sale data into the models from 20160801 to 20170815. Besides, we dummy all the categorical features such as the state, city, store type and cluster and use them as new features. The model we've tired in this part including Random Forest, Bagging, XGBoost as well as MLP. The plot below descirbes the feature performance.</p>
<p><img src="https://favorita-ee379k.github.io/MLgraph/performance.png" alt="performance"></p>
<p>From the picture, we can see that XGBoost is still the best among the four machine learning models but they all get a score of more than 1, twice as large as the Moving Average's score and even larger than the last year model. The poor performance is because of the feature enginnering we made was not good. The date itself does not contain enough information, and feeding all the raw time series data will lead to overfitting. Besides, these four models need a huge amount of time to tune and train given the amount of training data. We need to find a faster model first before further feature Engineering.</p>
<h3 id="2.-LGBM-is-faster">2. LGBM is faster<a class="anchor-link" href="#2.-LGBM-is-faster">&#182;</a></h3><p>LGBM is short for Light Gradient Boosting Method. Different from other boosing methods, this model splits the trees leaf-wise instead of level-wise, this allows LGBM to minimise loss faster and thus gain more computation speed. The pictures below describes the concept well.</p>
<p><img src="https://favorita-ee379k.github.io/MLgraph/leaf-wise.png" alt="leaf-wise">
<img src="https://favorita-ee379k.github.io/MLgraph/level-wise.png" alt="level-wise"></p>
<p>With the help of LGBM, we get more time and more patience to do more feature engineering.</p>
<h3 id="3.-Feature-Engineering">3. Feature Engineering<a class="anchor-link" href="#3.-Feature-Engineering">&#182;</a></h3><p>With the inspiration of moving average and smoothing method, we begin to view time series as a combination of trend, seasonality and noise. We want to capture the trend and seasonality instead of noise. First, for the trend, we calculate the average unit sales of each item in a certain store. The time period we used was firstly 7 days, 14 days, 28 days as well as 56 days, since this will capture the weekly mean as well as monthly trend. But after we did a FFT on the data we also discovered a period of 2 days and 3 days, so we add the 2-day mean and 3-day mean into our model too. We use these moving-average features to predict the following 16 days since the task is to predict the last 16 days' sale in August. In that way we capture the trend well. Next, to capture the seasonality, the 16 days of training data all begins on Wednesday, the same with the first day of the test set. We fit 16 LGBM model individually to the 16 days in our prepared training dataset. In this way we capture the short term seasonality well. However, in this way, our model only used the most information of 16 days and a small amount of information of 56 days. The way we use information is still insufficient. In order to cope with that, we created 4 training sets, begining at 6.28, 7.5, 7.12 and 7.19. For the yearly trend, we just ignored it, for we tried the same thing on 2016 and 2017 but after ensemble them the result got worse.</p>
<p>For transaction features, we used 7 day mean, 14 day mean and 28 day mean. For categorical features, we used store city and store cluster as well as onpromotion and amount of ompromotion in last 14 days. We also tried other features like oil price but it was total useless and ruined our result. The structure of feature engineering are shown below.</p>
<p><img src="https://favorita-ee379k.github.io/MLgraph/feature_structure.png" alt="feature_structure"></p>
<p>The feature importances of the first day and the last day in the training set are shown below.</p>
<p><img src="https://favorita-ee379k.github.io/MLgraph/day_1.png" alt="day_1">
<img src="https://favorita-ee379k.github.io/MLgraph/day_16.png" alt="day_2"></p>
<p>The following table allows to view the order change of the 16 days more clearly.
<img src="https://favorita-ee379k.github.io/MLgraph/feature_importance.png" alt="feature_importance">
We can find that as the number of days goes up, 7 day mean's importance decreases while 56 day mean's importance increases. This indicating that as when the predicting date is far away from our training data, the importance of long term trend goes up while the importance of short term trend goes down.</p>
<p>With this model, we reached a score of 0.520. After ensembled it with the result from moving average, we reached a score of 0.517, ranking 38/1126 in the competition.
<img src="https://favorita-ee379k.github.io/MLgraph/rank.png" alt="rank"></p>

</div>
</div>
</div>



    </div>
  </div>

  </div>





  <script src="http://nbviewer.jupyter.org/static/components/bootstrap/js/bootstrap.min.js?v=5869c96cc8f19086aee625d670d741f9"></script>
  <script src="http://nbviewer.jupyter.org/static/components/headroom.js/dist/headroom.min.js?v=b0a311ea668f8e768ea375f4a7abb81c"></script>
  <script src="http://nbviewer.jupyter.org/static/components/headroom.js/dist/jQuery.headroom.min.js?v=f3a1bae118315d0c234afc74dc6aab71"></script>

</body>
</html>